[Input Output]
checkpoint_dir = ./training
deepspeech_graph_fname = ./ds_graph/models/output_graph.pb
verts_mmaps_path = ./training_data/data_verts.npy
raw_audio_path = ./training_data/raw_audio_fixed.pkl
processed_audio_path = 
data2array_verts_path = ./training_data/subj_seq_to_idx.pkl

[Audio Parameters]
audio_feature_type = deepspeech
num_audio_features = 29
audio_window_size = 16
audio_window_stride = 1
condition_speech_features = False
speech_encoder_size_factor = 2.0

[Model Parameters]
num_vertices = 52
expression_dim = 128
init_expression = False
num_consecutive_frames = 2
absolute_reconstruction_loss = False
velocity_weight = 10.0
acceleration_weight = 0.0
verts_regularizer_weight = 0.0

[Data Setup]
subject_for_training = 1.mp4 2.mp4 3.mp4 4.mp4 5.mp4 6.mp4 7.mp4 8.mp4 9.mp4 10.mp4 

subject_for_validation = 11.mp4

[Learning Parameters]
batch_size = 64
learning_rate = 1e-3
decay_rate = 0.9
epoch_num = 30
adam_beta1_value = 0.9

[Visualization Parameters]
num_render_sequences = 3

